{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Include hyper parameter training and exeprimentation\n",
    "# 6) Start a discussion on the topic \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#import sys\n",
    "class MLP():\n",
    "\n",
    "    def __init__(self, numinputs=4, hiddenlayers=[4,2,2,1], numoutputs=1, activationfunc = [1,1,1], alpha = 0.25, lossfunc = 1):\n",
    "        self.numinputs = numinputs\n",
    "        self.hiddenlayers = hiddenlayers\n",
    "        self.numoutputs = numoutputs\n",
    "\n",
    "        # create a generic representation of the layers\n",
    "        self.layers = [numinputs] + hiddenlayers + [numoutputs]\n",
    "         \n",
    "        # create random connection weights for the layers\n",
    "        weights = []\n",
    "        bias = []\n",
    "        for i in range(len(self.layers)-1):\n",
    "            bias.append(np.ones(self.layers[i+1]))\n",
    "            weights.append(np.random.rand(self.layers[i], self.layers[i+1]))\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    \n",
    "        if lossfunc == 1: \n",
    "            self.lss = self.loss_func\n",
    "        elif lossfunc == 2: \n",
    "            self.lss = self.cross_entropy_loss\n",
    "        elif lossfunc == 3: \n",
    "             self.lss = self.binary_cross_entropy_loss\n",
    "    \n",
    "        self.activationfun = [np.vectorize(self._sigmoid) if i == 1 else np.vectorize(self._relu) if i == 2   else np.vectorize(self._tanh)   for i in activationfunc]\n",
    "        self.activationfunderv = [np.vectorize(self._sigmoid_derv) if i == 1 else np.vectorize(self._relu_derv) if i == 2   else np.vectorize(self._tanh_derv)  for i in activationfunc]\n",
    "\n",
    "# #         #Test\n",
    "#         self.weights = [np.array([[-0.2, -0.1],\n",
    "#         [ 0.2,  0.3]]), np.array([[0.2],\n",
    "#         [0.3]])]\n",
    "#         self.bias = [np.array([0.1, 0.1]), np.array([0.2])]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def transform_vector(self,weights_trial):\n",
    "        weights = []\n",
    "        index = 0\n",
    "        for i in range(len(self.layers)-1):\n",
    "            count = self.layers[i]*self.layers[i+1]\n",
    "            weights.append(np.array(weights_trial[index:count+index]).reshape(self.layers[i+1],self.layers[i]).transpose())\n",
    "            index+=count\n",
    "        self.weights = weights\n",
    "       \n",
    "        return 1 \n",
    "    \n",
    "\n",
    "\n",
    "    def weight_update(self,weight_update):\n",
    "        self.weights = [  w-w_u for w,w_u in zip(self.weights, weight_update) ]\n",
    "        \n",
    "    \n",
    "    def bias_update(self,bias_update): \n",
    "        self.bias = [  b-b_u for b,b_u in zip(self.bias, bias_update) ]\n",
    "\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "    \n",
    "    def _relu(self, x): \n",
    "        return max(0, x)\n",
    "    \n",
    "    def _tanh(self, x): \n",
    "         return (np.exp(x) - np.exp(-x))/ (np.exp(x)+ np.exp(-x))\n",
    "        \n",
    "        \n",
    "    def _sigmoid_derv(self, x): \n",
    "             val = self._sigmoid(x) \n",
    "             return val * (1-val)\n",
    "       \n",
    "\n",
    "    def _relu_derv(self, x): \n",
    "        return 1 if x>0 else 0\n",
    "    \n",
    "    def _tanh_derv(self, x):\n",
    "        return 1 - (((np.exp(x) - np.exp(-x))**2)/ ((np.exp(x)+ np.exp(-x))**2))\n",
    "\n",
    "    def loss_func(self, x,y): \n",
    "        #add aquare root\n",
    "        fun = np.vectorize(math.sqrt)\n",
    "        return (y-x)**2\n",
    "    \n",
    "    \n",
    "    def cross_entropy_loss(self,pred, y):\n",
    "        if y == 1:\n",
    "            return -np.log(pred)\n",
    "        else:\n",
    "            return -np.log(1 - pred)\n",
    "        \n",
    "    def binary_cross_entropy_loss(self,pred,y): \n",
    "        return -((y*np.log(pred)) + (1-y) * np.log(1-pred))\n",
    "  \n",
    "\n",
    "    def apply_loss(x,y): \n",
    "        return self.lss(x,y)\n",
    "        \n",
    "    def forward_propagate(self,inputs):\n",
    "        activations = inputs\n",
    "        net_inputs = []\n",
    "        z_value = []\n",
    "        activated_value = []\n",
    "        activated_value.append(inputs)\n",
    "       \n",
    "        for w,b,a in zip(self.weights,self.bias,self.activationfun):\n",
    "            net_inputs = (np.dot(activations, w)) + b\n",
    "            activations = a(net_inputs)\n",
    "            z_value.append(net_inputs)\n",
    "            activated_value.append(activations)\n",
    "\n",
    "        return z_value, activated_value\n",
    "    \n",
    "\n",
    "    def back_propogate(self,z_value, activated_value,  y,step):\n",
    "        eval =  self.lss(activated_value[-1],y)\n",
    "        weight_update = []\n",
    "        bias_update = []\n",
    "        sig_derv_activ = []\n",
    "        for i in range(len(self.weights)-1, -1,-1):\n",
    "                \n",
    "                sig_derv_activ = self.activationfunderv[i](z_value[i])*eval\n",
    "                weight_updt = self.alpha* (1.0/step)*np.sum(np.array([np.dot(np.reshape(sig_derv_activ[x],(-1,1)),np.reshape(activated_value[i][x],(-1,1)).transpose()) for x in range((len(activated_value[i])))]),axis=0)\n",
    "\n",
    "                bias_updt = np.average(sig_derv_activ,axis = 0)* self.alpha\n",
    "                eval = [np.dot(np.reshape(sig_derv_activ[x], (-1, 1)).transpose(),self.weights[i].transpose()) [0]for x in range(len(sig_derv_activ))]\n",
    "                weight_update.insert(0,weight_updt.transpose())\n",
    "                bias_update.insert(0,bias_updt)\n",
    "        return weight_update, bias_update\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict_values(self, inputs, y): \n",
    "            z_value, activated_value = self.forward_propagate(np.array(inputs))\n",
    "            \n",
    "            loss = self.lss(activated_value[-1],np.reshape(np.array(y),(-1,1)))\n",
    "            cost = (1.0/len(y))*sum(loss)\n",
    "\n",
    "            return  cost\n",
    "        \n",
    "        \n",
    "\n",
    "def training(df_X,hiddenlayers,numoutputs,activationfunc,alpha,loss,step, epochs,Y_train):\n",
    "    print('Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH')\n",
    "    print('Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy')\n",
    "    print('#########')\n",
    "    print('Results for hyperparameter:')\n",
    "    print('Epochs:' +str(epochs)) \n",
    "    print('Steps:' + str(step))\n",
    "    print('Learning rate:' +str(alpha))\n",
    "    print('Number of hidden layers:'+str(len(hiddenlayers)) )\n",
    "    print('Loss function:'+str(lossfunc))\n",
    "    print('Activation functions:'+str(activationfunc))\n",
    "    mlp = MLP(df_X.shape[1], hiddenlayers, numoutputs,activationfunc, alpha, lossfunc)\n",
    "\n",
    "     # Need to add this as arg as well and epochs\n",
    "    for i in range(epochs):\n",
    "        for i in range(1, sample_len,step): #sample_len,5): \n",
    "            z_value, activated_value =mlp.forward_propagate(np.array(X_train.iloc[0:i]))\n",
    "            weight_update, bias_update= mlp.back_propogate(z_value,activated_value, np.array(Y_train.iloc[0:i]).reshape(i,1),step)\n",
    "            mlp.weight_update(weight_update)\n",
    "            mlp.bias_update(bias_update)\n",
    "    return mlp\n",
    "           \n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "# #     df = pd.read_csv('/Users/smiroshnikova/Desktop/trial.csv',header=None)\n",
    "#     df = pd.read_csv('data_banknote_authentication.csv',header=None)\n",
    "\n",
    "#     df_Y = df.iloc[:,-1]\n",
    "#     df_X = df.iloc[:,0:-1]\n",
    "#     X_train,X_test,Y_train,Y_test=train_test_split(df_X,df_Y,test_size=0.15)\n",
    "#     sample_len = len(X_train)\n",
    "#     epochs = 1\n",
    "#     hiddenlayers=[2,2]\n",
    "#     numoutputs=1\n",
    "#     activationfunc = [1,1,1]\n",
    "#     alpha = 0.2\n",
    "#     lossfunc =3\n",
    "#     step = 5 \n",
    "#     epochs = 500\n",
    "\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSO for MLP Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "import itertools    \n",
    "import random  \n",
    "\n",
    "\n",
    "\n",
    "class particle(): \n",
    "    def __init__(self,x):\n",
    "        self.velocity = []\n",
    "        self.position = x\n",
    "        self.best_position = []\n",
    "        self.best_error = -1\n",
    "        self.error = -1\n",
    "        self.inf_best_error = -1\n",
    "        self.inf_best_position = []\n",
    "        self.w=0.5 \n",
    "        self.num_dimensions = len(x)\n",
    "        for i in range(0,self.num_dimensions):\n",
    "            self.velocity.append(random.uniform(-1,1))\n",
    "        \n",
    "            \n",
    "\n",
    "    \n",
    "    def evaluate(self,costFunc):\n",
    "        \n",
    "        self.err=costFunc(self.position)\n",
    "        if  self.best_error==-1 or self.error < self.best_error:\n",
    "            self.pos_best=self.position\n",
    "            self.best_error=self.error\n",
    "    \n",
    "    def update_velocity(self,post_best_g): \n",
    "        for i in range(0,self.num_dimensions):\n",
    "            r1=random.random()\n",
    "            r2=random.random()\n",
    "            r3=random.random()\n",
    "\n",
    "            vel_personal=r1*(self.pos_best[i]-self.position[i])\n",
    "            vel_global=r2*(post_best_g[i]-self.position[i])\n",
    "            vel_informant=r3*(self.inf_best_position[i])-self.position[i]\n",
    "            self.velocity[i]=self.w*self.velocity[i]+vel_personal+vel_global+vel_informant\n",
    "        \n",
    "    \n",
    "    def update_position(self): \n",
    "           for i in range(0,self.num_dimensions):\n",
    "                self.position[i]=self.position[i]+self.velocity[i]\n",
    "                \n",
    "        \n",
    "class PSO():\n",
    "    def __init__(self,costFunc,samples,num_particles,max_iterations,max_informants):\n",
    "        num_dimensions=len(samples[0])\n",
    "        err_best_g=-1                   \n",
    "        pos_best_g=[]                  \n",
    "\n",
    "        swarm=[]\n",
    "        swarm_informants=[]\n",
    "       \n",
    "    \n",
    "\n",
    "        for i in range(0,num_particles):\n",
    "            swarm.append(particle(samples[i]))\n",
    "    \n",
    "        for i in range(0,num_particles): \n",
    "            swarm_informants.append(random.sample(swarm[0:i]+swarm[i+1:num_particles],max_informants))\n",
    "        \n",
    "        \n",
    "        for i in range(0,max_iterations):\n",
    "#             print('Iteration no:' + str(i))\n",
    "            \n",
    "            for j in range(0,num_particles):\n",
    "#                 print('Particle no:' + str(j))\n",
    "                swarm[j].evaluate(costFunc)\n",
    "\n",
    "                # determine if current particle is the best (globally)\n",
    "                if  ( err_best_g == -1 or swarm[j].err < err_best_g ) and not math.isnan(err_best_g)  :\n",
    "                    \n",
    "                    pos_best_g=list(swarm[j].position)\n",
    "                    err_best_g=float(swarm[j].err)\n",
    "                    print(err_best_g)\n",
    "                    \n",
    "                \n",
    "                # determine the best amongst the informants for each particle\n",
    "                for x in range(0,max_informants):\n",
    "                    if swarm[j].inf_best_error == -1 or swarm[j].inf_best_error>swarm_informants[x].err :\n",
    "                        swarm[j].inf_best_error=float(swarm_informants[j][x].error)\n",
    "                        swarm[j].inf_best_position=list(swarm_informants[j][x].position)\n",
    "                    \n",
    "\n",
    "            # cycle through swarm and update velocities and position\n",
    "            for j in range(0,num_particles):\n",
    "                swarm[j].update_velocity(pos_best_g)\n",
    "                swarm[j].update_position()\n",
    " \n",
    "\n",
    "        # print final results\n",
    "        print 'FINAL:'\n",
    "        print pos_best_g\n",
    "        print err_best_g\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight(layers):\n",
    "    weight = []\n",
    "    for i in range(len(layers)-1):\n",
    "            x = np.random.rand(layers[i], layers[i+1])\n",
    "            weight.extend(np.concatenate(x.transpose()))\n",
    "    return weight\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing PSO\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/smiroshnikova/Desktop/CW1/data_banknote_authentication.csv',header=None)\n",
    "\n",
    "df_Y = df.iloc[:,-1]\n",
    "df_X = df.iloc[:,0:-1]\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(df_X,df_Y,test_size=0.15)\n",
    "    \n",
    "hiddenlayers=[2,3,2]\n",
    "numoutputs=1\n",
    "activationfunc = [1,1,1,1]\n",
    "alpha = 0.25\n",
    "lossfunc =3\n",
    "step = 5 \n",
    "epochs = 500\n",
    "num_particles = 50\n",
    "max_iterations = 500\n",
    "max_informants = 6\n",
    "numinputs = X_train.shape[1]\n",
    "sample_len = 5\n",
    "\n",
    "weights = []\n",
    "for i in range(num_particles):\n",
    "    weights.append(generate_weight([numinputs] + hiddenlayers + [numoutputs]))\n",
    "    \n",
    "def cost_func(x):\n",
    "    mlp = MLP(numinputs, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc)\n",
    "\n",
    "    mlp.transform_vector(x)\n",
    "    \n",
    "    cost = mlp.predict_values(X_train,Y_train)\n",
    "    return cost\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "print('Executing PSO')\n",
    "print('--------------------')\n",
    "start_time = time.time()\n",
    "PSO(cost_func,weights,num_particles,max_iterations,max_informants)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print('Executing MLP')\n",
    "print('--------------------')\n",
    "mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, step, epochs,Y_train)\n",
    "cost = mlp.predict_values(X_test,Y_test)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tunning for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_test = []\n",
    "for i in range(0, 10000, 2500): \n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, step, i,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        epoch_test.append((i,cost))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_test = []\n",
    "for i in range(1,len(X_train), 20): \n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, i, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        steps_test.append((i,cost))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_static_test = []\n",
    "for i in range(1,4):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,[i for x in range(len(hiddenlayers) + 1)], alpha, lossfunc, step, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        activation_static_test.append((i,cost))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_test = []\n",
    "for i in np.arange(0,1, 0.10):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , i, lossfunc, step, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        alpha_test.append((i,cost))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunc_test = []\n",
    " \n",
    "for i in range(1,3):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, i, step, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        lossfunc_test.append((i,cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    from sympy.utilities.iterables import multiset_permutations\n",
    "    import numpy as np\n",
    "\n",
    "    perms = []\n",
    "    perms_test = []\n",
    "    for i in range(1,6): \n",
    "\n",
    "        for p in multiset_permutations(np.random.permutation(i)+1):\n",
    "            perms.append(p)\n",
    "\n",
    "    for i in perms: \n",
    "        \n",
    "        mlp = training(X_train, i, numoutputs,[1]*(len(i)+1) , alpha, lossfunc, step, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        perms_test.append((i,cost))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

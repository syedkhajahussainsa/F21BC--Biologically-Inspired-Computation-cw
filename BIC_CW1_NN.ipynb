{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215067]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:0\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.33985899]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:5000\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215854]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:1\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215903]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:21\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44200675]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:41\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44154211]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:61\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44113673]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:81\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44044608]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:101\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.4403833]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:121\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.43819324]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:141\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.43804647]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:161\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.43870929]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:181\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.43842232]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:201\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.43817776]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215065]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[2, 2, 2]\n",
      "#########\n",
      "Cost:[0.44215938]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[3, 3, 3]\n",
      "#########\n",
      "Cost:[2.32647648]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.0\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.4169579]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.1\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44213683]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.2\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44214842]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.30000000000000004\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215203]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.4\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.442154]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.5\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.4421551]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.6000000000000001\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215576]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.7000000000000001\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215623]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.8\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215667]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.9\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########\n",
      "Cost:[0.44215698]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215063]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.4421506]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.44216949]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215056]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44214256]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:3\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215332]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:3\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215069]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:3\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215312]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:3\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44214256]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:3\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215049]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:3\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.4421426]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215483]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215353]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215475]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215075]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215342]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215074]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215482]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215349]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215452]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44214261]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215323]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44214262]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215473]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215073]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215447]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########\n",
      "Cost:[0.44214262]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215065]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44214261]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215338]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215073]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215322]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44214262]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215059]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:4\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44214261]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215574]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215493]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215569]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215354]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215487]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215352]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215573]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215493]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215559]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215076]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215479]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215075]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215567]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215354]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215557]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215076]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44215344]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########\n",
      "Cost:[0.44215073]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:5\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-494f5627dc2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mperms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumoutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mpred_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mlossfunc_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-494f5627dc2c>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(df_X, hiddenlayers, numoutputs, activationfunc, alpha, loss, step, epochs, Y_train)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#sample_len,5):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mz_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivated_value\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mweight_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_update\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propogate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivated_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mweight_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_update\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propogate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivated_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-494f5627dc2c>\u001b[0m in \u001b[0;36mforward_propagate\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mnet_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mz_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mactivated_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/smiroshnikova/opt/anaconda2/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         \u001b[0;34m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mexcluded\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2067\u001b[0m         \"\"\"\n\u001b[0;32m-> 2068\u001b[0;31m         \u001b[0mexcluded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexcluded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2069\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexcluded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2070\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#import sys\n",
    "class MLP():\n",
    "\n",
    "    def __init__(self, numinputs=4, hiddenlayers=[4,2,2,1], numoutputs=1, activationfunc = [1,1,1], alpha = 0.25, lossfunc = 1):\n",
    "        self.numinputs = numinputs\n",
    "        self.hiddenlayers = hiddenlayers\n",
    "        self.numoutputs = numoutputs\n",
    "\n",
    "        # create a generic representation of the layers\n",
    "        layers = [numinputs] + hiddenlayers + [numoutputs]\n",
    "         \n",
    "        # create random connection weights for the layers\n",
    "        weights = []\n",
    "        bias = []\n",
    "        for i in range(len(layers)-1):\n",
    "            bias.append(np.ones(layers[i+1]))\n",
    "            weights.append(np.random.rand(layers[i], layers[i+1]))\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        \n",
    "        if lossfunc == 1: \n",
    "            self.lss = self.loss_func\n",
    "        elif lossfunc == 2: \n",
    "            self.lss = self.cross_entropy_loss\n",
    "    \n",
    "        self.activationfun = [np.vectorize(self._sigmoid) if i == 1 else np.vectorize(self._relu) if i == 2   else np.vectorize(self._tanh)   for i in activationfunc]\n",
    "        self.activationfunderv = [np.vectorize(self._sigmoid_derv) if i == 1 else np.vectorize(self._relu_derv) if i == 2   else np.vectorize(self._tanh_derv)  for i in activationfunc]\n",
    "\n",
    "# #         #Test\n",
    "#         self.weights = [np.array([[-0.2, -0.1],\n",
    "#         [ 0.2,  0.3]]), np.array([[0.2],\n",
    "#         [0.3]])]\n",
    "#         self.bias = [np.array([0.1, 0.1]), np.array([0.2])]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def weight_update(self,weight_update):\n",
    "        self.weights = [  w-w_u for w,w_u in zip(self.weights, weight_update) ]\n",
    "        \n",
    "    \n",
    "    def bias_update(self,bias_update): \n",
    "        self.bias = [  b-b_u for b,b_u in zip(self.bias, bias_update) ]\n",
    "\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "    \n",
    "    def _relu(self, x): \n",
    "        return max(0, x)\n",
    "    \n",
    "    def _tanh(self, x): \n",
    "         return (np.exp(x) - np.exp(-x))/ (np.exp(x)+ np.exp(-x))\n",
    "        \n",
    "        \n",
    "    def _sigmoid_derv(self, x): \n",
    "             val = self._sigmoid(x) \n",
    "             return val * (1-val)\n",
    "       \n",
    "\n",
    "    def _relu_derv(self, x): \n",
    "        return 1 if x>0 else 0\n",
    "    \n",
    "    def _tanh_derv(self, x):\n",
    "        return 1 - (((np.exp(x) - np.exp(-x))**2)/ ((np.exp(x)+ np.exp(-x))**2))\n",
    "\n",
    "    def loss_func(self, x,y): \n",
    "        #add aquare root\n",
    "        fun = np.vectorize(math.sqrt)\n",
    "        return (y-x)**2\n",
    "    \n",
    "    \n",
    "    def cross_entropy_loss(self,pred, y):\n",
    "        if y == 1:\n",
    "            return -np.log(pred)\n",
    "        else:\n",
    "            return -np.log(1 - pred)\n",
    "    \n",
    "    def forward_propagate(self,inputs):\n",
    "        activations = inputs\n",
    "        net_inputs = []\n",
    "        z_value = []\n",
    "        activated_value = []\n",
    "        activated_value.append(inputs)\n",
    "        \n",
    "        for w,b,a in zip(self.weights,self.bias,self.activationfun):\n",
    "            \n",
    "            net_inputs = (np.dot(activations, w)) + b\n",
    "            activations = a(net_inputs)\n",
    "            z_value.append(net_inputs)\n",
    "            activated_value.append(activations)\n",
    "\n",
    "        return z_value, activated_value\n",
    "    \n",
    "\n",
    "    def back_propogate(self,z_value, activated_value,  y,step):\n",
    "        eval =  self.lss(activated_value[-1],y)\n",
    "        weight_update = []\n",
    "        bias_update = []\n",
    "        sig_derv_activ = []\n",
    "        for i in range(len(self.weights)-1, -1,-1):\n",
    "                \n",
    "                sig_derv_activ = self.activationfunderv[i](z_value[i])*eval\n",
    "                weight_updt = self.alpha* (1.0/step)*np.sum(np.array([np.dot(np.reshape(sig_derv_activ[x],(-1,1)),np.reshape(activated_value[i][x],(-1,1)).transpose()) for x in range((len(activated_value[i])))]),axis=0)\n",
    "\n",
    "                bias_updt = np.average(sig_derv_activ,axis = 0)* self.alpha\n",
    "                eval = [np.dot(np.reshape(sig_derv_activ[x], (-1, 1)).transpose(),self.weights[i].transpose()) [0]for x in range(len(sig_derv_activ))]\n",
    "                weight_update.insert(0,weight_updt.transpose())\n",
    "                bias_update.insert(0,bias_updt)\n",
    "        return weight_update, bias_update\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict_values(self, inputs, y): \n",
    "        z_value, activated_value = mlp.forward_propagate(np.array(inputs))\n",
    "        loss = self.lss(activated_value[-1],np.reshape(np.array(y),(-1,1)))\n",
    "        cost = (1.0/len(y))*sum(loss)\n",
    "        print('#########')\n",
    "        print('Cost:'+str(cost))\n",
    "        return activated_value[-1],loss, cost\n",
    "\n",
    "def training(df_X,hiddenlayers,numoutputs,activationfunc,alpha,loss,step, epochs,Y_train):\n",
    "    print('Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH')\n",
    "    print('Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy')\n",
    "    print('#########')\n",
    "    print('Results for hyperparameter:')\n",
    "    print('Epochs:' +str(epochs)) \n",
    "    print('Steps:' + str(step))\n",
    "    print('Learning rate:' +str(alpha))\n",
    "    print('Number of hidden layers:'+str(len(hiddenlayers)) )\n",
    "    print('Loss function:'+str(lossfunc))\n",
    "    print('Activation functions:'+str(activationfunc))\n",
    "    mlp = MLP(df_X.shape[1], hiddenlayers, numoutputs,activationfunc, alpha, lossfunc)\n",
    "\n",
    "     # Need to add this as arg as well and epochs\n",
    "    for i in range(epochs):\n",
    "        for i in range(1, sample_len,step): #sample_len,5): \n",
    "            z_value, activated_value =mlp.forward_propagate(np.array(X_train.iloc[0:i]))\n",
    "            weight_update, bias_update= mlp.back_propogate(z_value,activated_value, np.array(Y_train.iloc[0:i]).reshape(i,1),step)\n",
    "            weight_update, bias_update= mlp.back_propogate(z_value,activated_value, np.array(Y_train.iloc[0:i]).reshape(i,1),step)\n",
    "            mlp.weight_update(weight_update)\n",
    "            mlp.bias_update(bias_update)\n",
    "    return mlp\n",
    "           \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "#     df = pd.read_csv('/Users/smiroshnikova/Desktop/trial.csv',header=None)\n",
    "    df = pd.read_csv('/Users/smiroshnikova/Desktop/data_banknote_authentication.csv',header=None)\n",
    "\n",
    "    df_Y = df.iloc[:,-1]\n",
    "    df_X = df.iloc[:,0:-1]\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(df_X,df_Y,test_size=0.15)\n",
    "    sample_len = len(X_train)\n",
    "    epochs = 1\n",
    "    #\"[1,2,3] 0.25 [1,2,1] 1\" input format\n",
    "    \"\"\"b=sys.argv[1]\n",
    "    c=b.split(\"[\")\n",
    "    d=c[1].split(\"]\")\n",
    "    e=d[0].split(\",\")\n",
    "    hidden=[]\n",
    "    e=['1','2','3']\n",
    "    for i in range (len(e)):\n",
    "        hidden.append(int(e[i]))\n",
    "\n",
    "    learningrate=float(sys.argv[2])\n",
    "    b1=sys.argv[3]\n",
    "    f=b1.split(\"[\")\n",
    "    g=f[1].split(\"]\")\n",
    "    h=g[0].split(\",\")\n",
    "    activation=[]\n",
    "    for j in range (len(h)):\n",
    "        activation.append(int(h[j]))\n",
    "    lossfunction=int(sys.argv[4])\n",
    "    print(hidden)\n",
    "    print(learningrate)\n",
    "    print(activation)\n",
    "    print(lossfunction)\n",
    "    \"\"\"\n",
    "    \n",
    "    hiddenlayers=[2,2]\n",
    "    numoutputs=1\n",
    "    activationfunc = [1,1,1]\n",
    "    alpha = 0.25\n",
    "    lossfunc =1\n",
    "    step = 5 \n",
    "    epochs = 500\n",
    "    mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, step, epochs,Y_train)\n",
    "    pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "    \n",
    "    epoch_test = []\n",
    "    for i in range(0, 10000, 5000): \n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, step, i,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        epoch_test.append((i,cost))\n",
    "    \n",
    "    steps_test = []\n",
    "    for i in range(1,len(X_train), 20): \n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, i, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        steps_test.append((i,cost))\n",
    "        \n",
    "    activation_static_test = []\n",
    "    for i in range(1,4):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,[i for x in range(len(hiddenlayers) + 1)], alpha, lossfunc, step, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        activation_static_test.append((i,cost))\n",
    "        alpha_test = []\n",
    "\n",
    "    for i in np.arange(0,1, 0.10):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , i, lossfunc, step, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        alpha_test.append((i,cost))\n",
    "    lossfunc_test = []\n",
    "    \n",
    "    for i in range(1,3):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, i, step, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        lossfunc_test.append((i,cost))\n",
    "\n",
    "    from sympy.utilities.iterables import multiset_permutations\n",
    "    import numpy as np\n",
    "\n",
    "    perms = []\n",
    "    for i in range(1,6): \n",
    "\n",
    "        for p in multiset_permutations(np.random.permutation(i)+1):\n",
    "            perms.append(p)\n",
    "\n",
    "    for i in perms: \n",
    "        \n",
    "        mlp = training(X_train, i, numoutputs,[1]*(len(i)+1) , alpha, lossfunc, step, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        lossfunc_test.append((i,cost))\n",
    "\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(*zip(*steps_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(*zip(*activation_static_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(*zip(*alpha_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actually permutations\n",
    "lossfunc_main = lossfunc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:0\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.42240636]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:2500\n",
      "Steps:5\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "epoch_test = []\n",
    "for i in range(0, 10000, 2500): \n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, step, i,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        epoch_test.append((i,cost))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_test = []\n",
    "for i in range(1,len(X_train), 20): \n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, i, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        steps_test.append((i,cost))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_static_test = []\n",
    "for i in range(1,4):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,[i for x in range(len(hiddenlayers) + 1)], alpha, lossfunc, step, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        activation_static_test.append((i,cost))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_test = []\n",
    "for i in np.arange(0,1, 0.10):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , i, lossfunc, step, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        alpha_test.append((i,cost))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunc_test = []\n",
    " \n",
    "for i in range(1,3):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, i, step, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        lossfunc_test.append((i,cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    from sympy.utilities.iterables import multiset_permutations\n",
    "    import numpy as np\n",
    "\n",
    "    perms = []\n",
    "    perms_test = []\n",
    "    for i in range(1,6): \n",
    "\n",
    "        for p in multiset_permutations(np.random.permutation(i)+1):\n",
    "            perms.append(p)\n",
    "\n",
    "    for i in perms: \n",
    "        \n",
    "        mlp = training(X_train, i, numoutputs,[1]*(len(i)+1) , alpha, lossfunc, step, epochs,Y_train)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        perms_test.append((i,cost))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

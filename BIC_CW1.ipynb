{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44314061]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:0\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.37142606]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:5000\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44314711]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:1\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.44314469]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:21\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 14 into shape (21,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8c9f00b3c7f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0msteps_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddenlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumoutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivationfunc\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mpred_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0msteps_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8c9f00b3c7f7>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(df_X, hiddenlayers, numoutputs, activationfunc, alpha, loss, step, epochs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#sample_len,5):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mz_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivated_value\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mweight_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_update\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propogate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivated_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 14 into shape (21,1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "class MLP():\n",
    "\n",
    "    def __init__(self, numinputs=4, hiddenlayers=[4,2,2,1], numoutputs=1, activationfunc = [1,1,1], alpha = 0.25, lossfunc = 1):\n",
    "        self.numinputs = numinputs\n",
    "        self.hiddenlayers = hiddenlayers\n",
    "        self.numoutputs = numoutputs\n",
    "\n",
    "        # create a generic representation of the layers\n",
    "        layers = [numinputs] + hiddenlayers + [numoutputs]\n",
    "         \n",
    "        # create random connection weights for the layers\n",
    "        weights = []\n",
    "        bias = []\n",
    "        for i in range(len(layers)-1):\n",
    "            bias.append(np.ones(layers[i+1]))\n",
    "            weights.append(np.random.rand(layers[i], layers[i+1]))\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        \n",
    "        if lossfunc == 1: \n",
    "            self.lss = self.loss_func\n",
    "        elif lossfunc == 2: \n",
    "            self.lss = self.cross_entropy_loss\n",
    "    \n",
    "        self.activationfun = [np.vectorize(self._sigmoid) if i == 1 else np.vectorize(self._relu) if i == 2   else np.vectorize(self._tanh)   for i in activationfunc]\n",
    "        self.activationfunderv = [np.vectorize(self._sigmoid_derv) if i == 1 else np.vectorize(self._relu_derv) if i == 2   else np.vectorize(self._tanh_derv)  for i in activationfunc]\n",
    "\n",
    "# #         #Test\n",
    "#         self.weights = [np.array([[-0.2, -0.1],\n",
    "#         [ 0.2,  0.3]]), np.array([[0.2],\n",
    "#         [0.3]])]\n",
    "#         self.bias = [np.array([0.1, 0.1]), np.array([0.2])]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def weight_update(self,weight_update):\n",
    "        self.weights = [  w-w_u for w,w_u in zip(self.weights, weight_update) ]\n",
    "        \n",
    "    \n",
    "    def bias_update(self,bias_update): \n",
    "        self.bias = [  b-b_u for b,b_u in zip(self.bias, bias_update) ]\n",
    "\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "    \n",
    "    def _relu(self, x): \n",
    "        return max(0, x)\n",
    "    \n",
    "    def _tanh(self, x): \n",
    "         return (np.exp(x) - np.exp(-x))/ (np.exp(x)+ np.exp(-x))\n",
    "        \n",
    "        \n",
    "    def _sigmoid_derv(self, x): \n",
    "             val = self._sigmoid(x) \n",
    "             return val * (1-val)\n",
    "       \n",
    "\n",
    "    def _relu_derv(self, x): \n",
    "        return 1 if x>0 else 0\n",
    "    \n",
    "    def _tanh_derv(self, x):\n",
    "        return 1 - (((np.exp(x) - np.exp(-x))**2)/ ((np.exp(x)+ np.exp(-x))**2))\n",
    "\n",
    "    def loss_func(self, x,y): \n",
    "        #add aquare root\n",
    "        fun = np.vectorize(math.sqrt)\n",
    "        return (y-x)**2\n",
    "    \n",
    "    \n",
    "    def cross_entropy_loss(self,pred, y):\n",
    "        if y == 1:\n",
    "            return -np.log(pred)\n",
    "        else:\n",
    "            return -np.log(1 - pred)\n",
    "    \n",
    "    def forward_propagate(self,inputs):\n",
    "        activations = inputs\n",
    "        net_inputs = []\n",
    "        z_value = []\n",
    "        activated_value = []\n",
    "        activated_value.append(inputs)\n",
    "        \n",
    "        for w,b,a in zip(self.weights,self.bias,self.activationfun):\n",
    "            \n",
    "            net_inputs = (np.dot(activations, w)) + b\n",
    "            activations = a(net_inputs)\n",
    "            z_value.append(net_inputs)\n",
    "            activated_value.append(activations)\n",
    "\n",
    "        return z_value, activated_value\n",
    "    \n",
    "\n",
    "    def back_propogate(self,z_value, activated_value,  y,step):\n",
    "        eval =  self.lss(activated_value[-1],y)\n",
    "        weight_update = []\n",
    "        bias_update = []\n",
    "        sig_derv_activ = []\n",
    "        for i in range(len(self.weights)-1, -1,-1):\n",
    "                \n",
    "                sig_derv_activ = self.activationfunderv[i](z_value[i])*eval\n",
    "                weight_updt = self.alpha* (1.0/step)*np.sum(np.array([np.dot(np.reshape(sig_derv_activ[x],(-1,1)),np.reshape(activated_value[i][x],(-1,1)).transpose()) for x in range((len(activated_value[i])))]),axis=0)\n",
    "\n",
    "                bias_updt = np.average(sig_derv_activ,axis = 0)* self.alpha\n",
    "                eval = [np.dot(np.reshape(sig_derv_activ[x], (-1, 1)).transpose(),self.weights[i].transpose()) [0]for x in range(len(sig_derv_activ))]\n",
    "                weight_update.insert(0,weight_updt.transpose())\n",
    "                bias_update.insert(0,bias_updt)\n",
    "        return weight_update, bias_update\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict_values(self, inputs, y): \n",
    "        z_value, activated_value = mlp.forward_propagate(np.array(inputs))\n",
    "        loss = self.lss(activated_value[-1],np.reshape(np.array(y),(-1,1)))\n",
    "        cost = (1.0/len(y))*sum(loss)\n",
    "        print('#########')\n",
    "        print('Cost:'+str(cost))\n",
    "        return activated_value[-1],loss, cost\n",
    "\n",
    "def training(df_X,hiddenlayers,numoutputs,activationfunc,alpha,loss,step, epochs):\n",
    "    print('Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH')\n",
    "    print('Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy')\n",
    "    print('#########')\n",
    "    print('Results for hyperparameter:')\n",
    "    print('Epochs:' +str(epochs)) \n",
    "    print('Steps:' + str(step))\n",
    "    print('Learning rate:' +str(alpha))\n",
    "    print('Number of hidden layers:'+str(len(hiddenlayers)) )\n",
    "    print('Loss function:'+str(lossfunc))\n",
    "    print('Activation functions:'+str(activationfunc))\n",
    "    mlp = MLP(df_X.shape[1], hiddenlayers, numoutputs,activationfunc, alpha, lossfunc)\n",
    "\n",
    "     # Need to add this as arg as well and epochs\n",
    "    for i in range(epochs):\n",
    "        for i in range(0, sample_len,step): #sample_len,5): \n",
    "            z_value, activated_value =mlp.forward_propagate(np.array(X_train.iloc[i:i+step]))            \n",
    "            weight_update, bias_update= mlp.back_propogate(z_value,activated_value, np.array(Y_train.iloc[i:i+step]).reshape(step,1),step)\n",
    "            mlp.weight_update(weight_update)\n",
    "            mlp.bias_update(bias_update)\n",
    "    return mlp\n",
    "           \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "#     df = pd.read_csv('/Users/smiroshnikova/Desktop/trial.csv',header=None)\n",
    "    df = pd.read_csv('/Users/smiroshnikova/Desktop/data_banknote_authentication.csv',header=None)\n",
    "\n",
    "    df_Y = df.iloc[:,-1]\n",
    "    df_X = df.iloc[:,0:-1]\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(df_X,df_Y,test_size=0.5)\n",
    "    sample_len = len(X_train)\n",
    "    epochs = 1\n",
    "    #\"[1,2,3] 0.25 [1,2,1] 1\" input format\n",
    "    \"\"\"b=sys.argv[1]\n",
    "    c=b.split(\"[\")\n",
    "    d=c[1].split(\"]\")\n",
    "    e=d[0].split(\",\")\n",
    "    hidden=[]\n",
    "    e=['1','2','3']\n",
    "    for i in range (len(e)):\n",
    "        hidden.append(int(e[i]))\n",
    "\n",
    "    learningrate=float(sys.argv[2])\n",
    "    b1=sys.argv[3]\n",
    "    f=b1.split(\"[\")\n",
    "    g=f[1].split(\"]\")\n",
    "    h=g[0].split(\",\")\n",
    "    activation=[]\n",
    "    for j in range (len(h)):\n",
    "        activation.append(int(h[j]))\n",
    "    lossfunction=int(sys.argv[4])\n",
    "    print(hidden)\n",
    "    print(learningrate)\n",
    "    print(activation)\n",
    "    print(lossfunction)\n",
    "    \"\"\"\n",
    "    hiddenlayers=[2,2]\n",
    "    numoutputs=1\n",
    "    activationfunc = [1,1,1]\n",
    "    alpha = 0.25\n",
    "    lossfunc =1\n",
    "    step = 2 # Need to add this as arg as well and epochs\n",
    "    epochs = 1000\n",
    "    mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, step, epochs)\n",
    "    pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "    \n",
    "#     epoch_test = []\n",
    "#     for i in range(0, 10000, 5000): \n",
    "#         mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, step, i)\n",
    "#         pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "#         epoch_test.append((i,cost))\n",
    "    \n",
    "#     steps_test = []\n",
    "#     for i in range(1,len(X_train), 20): \n",
    "#         mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, i, epochs)\n",
    "#         pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "#         steps_test.append((i,cost))\n",
    "        \n",
    "#     activation_static_test = []\n",
    "#     for i in range(1,4):\n",
    "#         mlp = training(X_train, hiddenlayers, numoutputs,[i for x in range(len(hiddenlayers) + 1)], alpha, lossfunc, step, epochs)\n",
    "#         pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "#         activation_static_test.append((i,cost))\n",
    "#         alpha_test = []\n",
    "\n",
    "#     for i in np.arange(0,1, 0.10):\n",
    "#         mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , i, lossfunc, step, epochs)\n",
    "#         pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "#         alpha_test.append((i,cost))\n",
    "#     lossfunc_test = []\n",
    "    \n",
    "#     for i in range(1,3):\n",
    "#         mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, i, step, epochs)\n",
    "#         pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "#         lossfunc_test.append((i,cost))\n",
    "\n",
    "#     from sympy.utilities.iterables import multiset_permutations\n",
    "#     import numpy as np\n",
    "\n",
    "#     perms = []\n",
    "#     for i in range(1,6): \n",
    "\n",
    "#         for p in multiset_permutations(np.random.permutation(i)+1):\n",
    "#             perms.append(p)\n",
    "\n",
    "#     for i in perms: \n",
    "        \n",
    "#         mlp = training(X_train, i, numoutputs,[1]*(len(i)+1) , alpha, lossfunc, step, epochs)\n",
    "#         pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "#         lossfunc_test.append((i,cost))\n",
    "\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6): \n",
    "\n",
    "    for p in multiset_permutations(np.random.permutation(i)+1):\n",
    "            perms.append(p)\n",
    "\n",
    "for i in perms: \n",
    "           for i in  \n",
    "                mlp = training(X_train, i, numoutputs,[1]*(len(i)+1) , alpha, lossfunc, step, epochs)\n",
    "                pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "                lossfunc_test.append((i,cost))\n",
    "                \n",
    "#new code\n",
    "def graphs (values):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    \n",
    "    for i in range len(values):\n",
    "        x.append[values[i][0]]\n",
    "        y.append[values[i][1]]\n",
    "    #df.plot(kind=\"scatter\",x=X, y=Y)\n",
    "    plt.plot(X,Y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:0\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n",
      "#########\n",
      "Cost:[0.42950714]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:5000\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:2\n",
      "Loss function:1\n",
      "Activation functions:[1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "epoch_test = []\n",
    "for i in range(0, 10000, 5000): \n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, step, i)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        epoch_test.append((i,cost))\n",
    "       #calling graphs functions\n",
    "        graphs(epoch_test)   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_test = []\n",
    "for i in range(1,len(X_train), 20): \n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, i, epochs)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        steps_test.append((i,cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_static_test = []\n",
    "for i in range(1,4):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,[i for x in range(len(hiddenlayers) + 1)], alpha, lossfunc, step, epochs)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        activation_static_test.append((i,cost))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_test = []\n",
    "\n",
    "for i in np.arange(0,1, 0.10):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , i, lossfunc, step, epochs)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        alpha_test.append((i,cost))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunc_test = []\n",
    "    \n",
    "for i in range(1,3):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, i, step, epochs)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        lossfunc_test.append((i,cost))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.utilities.iterables import multiset_permutations\n",
    "    import numpy as np\n",
    "\n",
    "    perms = []\n",
    "    for i in range(1,6): \n",
    "\n",
    "        for p in multiset_permutations(np.random.permutation(i)+1):\n",
    "            perms.append(p)\n",
    "\n",
    "    for i in perms: \n",
    "        \n",
    "        mlp = training(X_train, i, numoutputs,[1]*(len(i)+1) , alpha, lossfunc, step, epochs)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        lossfunc_test.append((i,cost))\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.89811763]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:0\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.06071502]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:5000\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.89967903]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:1\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.89853969]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.8979555]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[2, 2]\n",
      "#########\n",
      "Cost:[-inf]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[3, 3]\n",
      "#########\n",
      "Cost:[-0.096101]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.0\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.05322721]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.1\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.8947269]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.2\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.89715365]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.30000000000000004\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.89799654]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.4\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.89851382]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.5\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.89912784]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.6000000000000001\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[-0.09141296]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.7000000000000001\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.89937016]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.8\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.89949271]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.9\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.89951303]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.89810792]\n",
      "Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH\n",
      "Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy\n",
      "#########\n",
      "Results for hyperparameter:\n",
      "Epochs:1000\n",
      "Steps:2\n",
      "Learning rate:0.25\n",
      "Number of hidden layers:1\n",
      "Loss function:1\n",
      "Activation functions:[1, 1]\n",
      "#########\n",
      "Cost:[0.89815111]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#import sys\n",
    "class MLP():\n",
    "\n",
    "    def __init__(self, numinputs=4, hiddenlayers=[4,2,2,1], numoutputs=1, activationfunc = [1,1,1], alpha = 0.25, lossfunc = 1):\n",
    "        self.numinputs = numinputs\n",
    "        self.hiddenlayers = hiddenlayers\n",
    "        self.numoutputs = numoutputs\n",
    "\n",
    "        # create a generic representation of the layers\n",
    "        layers = [numinputs] + hiddenlayers + [numoutputs]\n",
    "         \n",
    "        # create random connection weights for the layers\n",
    "        weights = []\n",
    "        bias = []\n",
    "        for i in range(len(layers)-1):\n",
    "            bias.append(np.ones(layers[i+1]))\n",
    "            weights.append(np.random.rand(layers[i], layers[i+1]))\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        \n",
    "        if lossfunc == 1: \n",
    "            self.lss = self.loss_func\n",
    "        elif lossfunc == 2: \n",
    "            self.lss = self.cross_entropy_loss\n",
    "    \n",
    "        self.activationfun = [np.vectorize(self._sigmoid) if i == 1 else np.vectorize(self._relu) if i == 2   else np.vectorize(self._tanh)   for i in activationfunc]\n",
    "        self.activationfunderv = [np.vectorize(self._sigmoid_derv) if i == 1 else np.vectorize(self._relu_derv) if i == 2   else np.vectorize(self._tanh_derv)  for i in activationfunc]\n",
    "\n",
    "# #         #Test\n",
    "#         self.weights = [np.array([[-0.2, -0.1],\n",
    "#         [ 0.2,  0.3]]), np.array([[0.2],\n",
    "#         [0.3]])]\n",
    "#         self.bias = [np.array([0.1, 0.1]), np.array([0.2])]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def weight_update(self,weight_update):\n",
    "        self.weights = [  w-w_u for w,w_u in zip(self.weights, weight_update) ]\n",
    "        \n",
    "    \n",
    "    def bias_update(self,bias_update): \n",
    "        self.bias = [  b-b_u for b,b_u in zip(self.bias, bias_update) ]\n",
    "\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "    \n",
    "    def _relu(self, x): \n",
    "        return max(0, x)\n",
    "    \n",
    "    def _tanh(self, x): \n",
    "         return (np.exp(x) - np.exp(-x))/ (np.exp(x)+ np.exp(-x))\n",
    "        \n",
    "        \n",
    "    def _sigmoid_derv(self, x): \n",
    "             val = self._sigmoid(x) \n",
    "             return val * (1-val)\n",
    "       \n",
    "\n",
    "    def _relu_derv(self, x): \n",
    "        return 1 if x>0 else 0\n",
    "    \n",
    "    def _tanh_derv(self, x):\n",
    "        return 1 - (((np.exp(x) - np.exp(-x))**2)/ ((np.exp(x)+ np.exp(-x))**2))\n",
    "\n",
    "    def loss_func(self, x,y): \n",
    "        #add aquare root\n",
    "        return y-x\n",
    "    \n",
    "    \n",
    "    def cross_entropy_loss(self,pred, y):\n",
    "        if y == 1:\n",
    "            return -np.log(pred)\n",
    "        else:\n",
    "            return -np.log(1 - pred)\n",
    "    \n",
    "    def forward_propagate(self,inputs):\n",
    "        activations = inputs\n",
    "        net_inputs = []\n",
    "        z_value = []\n",
    "        activated_value = []\n",
    "        activated_value.append(inputs)\n",
    "        \n",
    "        for w,b,a in zip(self.weights,self.bias,self.activationfun):\n",
    "            \n",
    "            net_inputs = (np.dot(activations, w)) + b\n",
    "            activations = a(net_inputs)\n",
    "            z_value.append(net_inputs)\n",
    "            activated_value.append(activations)\n",
    "\n",
    "        return z_value, activated_value\n",
    "    \n",
    "\n",
    "    def back_propogate(self,z_value, activated_value,  y,step):\n",
    "        eval =  self.lss(activated_value[-1],y)\n",
    "        weight_update = []\n",
    "        bias_update = []\n",
    "        sig_derv_activ = []\n",
    "        for i in range(len(self.weights)-1, -1,-1):\n",
    "                \n",
    "                sig_derv_activ = self.activationfunderv[i](z_value[i])*eval\n",
    "                weight_updt = self.alpha* (1.0/step)*np.sum(np.array([np.dot(np.reshape(sig_derv_activ[x],(-1,1)),np.reshape(activated_value[i][x],(-1,1)).transpose()) for x in range((len(activated_value[i])))]),axis=0)\n",
    "\n",
    "                bias_updt = np.average(sig_derv_activ,axis = 0)* self.alpha\n",
    "                eval = [np.dot(np.reshape(sig_derv_activ[x], (-1, 1)).transpose(),self.weights[i].transpose()) [0]for x in range(len(sig_derv_activ))]\n",
    "                weight_update.insert(0,weight_updt.transpose())\n",
    "                bias_update.insert(0,bias_updt)\n",
    "        return weight_update, bias_update\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict_values(self, inputs, y): \n",
    "        z_value, activated_value = mlp.forward_propagate(np.array(inputs))\n",
    "        loss = self.lss(activated_value[-1],np.reshape(np.array(y),(-1,1)))\n",
    "        cost = (1.0/len(y))*sum(loss)\n",
    "        print('#########')\n",
    "        print('Cost:'+str(cost))\n",
    "        return activated_value[-1],loss, cost\n",
    "\n",
    "def training(df_X,hiddenlayers,numoutputs,activationfunc,alpha,loss,step, epochs):\n",
    "    print('Activation Functions are as follows: 1 -> Sigmoid | 2-> ReLu | 3-> tanH')\n",
    "    print('Loss Functions are as follows: 1 -> Error | 2-> Cross Entropy')\n",
    "    print('#########')\n",
    "    print('Results for hyperparameter:')\n",
    "    print('Epochs:' +str(epochs)) \n",
    "    print('Steps:' + str(step))\n",
    "    print('Learning rate:' +str(alpha))\n",
    "    print('Number of hidden layers:'+str(len(hiddenlayers)) )\n",
    "    print('Loss function:'+str(lossfunc))\n",
    "    print('Activation functions:'+str(activationfunc))\n",
    "    mlp = MLP(df_X.shape[1], hiddenlayers, numoutputs,activationfunc, alpha, lossfunc)\n",
    "\n",
    "     # Need to add this as arg as well and epochs\n",
    "    for i in range(epochs):\n",
    "        for i in range(0, sample_len,step): #sample_len,5): \n",
    "            z_value, activated_value =mlp.forward_propagate(np.array(X_train.iloc[i:i+step]))            \n",
    "            weight_update, bias_update= mlp.back_propogate(z_value,activated_value, np.array(Y_train.iloc[i:i+step]).reshape(step,1),step)\n",
    "            mlp.weight_update(weight_update)\n",
    "            mlp.bias_update(bias_update)\n",
    "    return mlp\n",
    "           \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df = pd.read_csv('/Users/smiroshnikova/Desktop/trial.csv',header=None)\n",
    "#     df = pd.read_csv('/Users/smiroshnikova/Desktop/data_banknote_authentication.csv',header=None)\n",
    "\n",
    "    df_Y = df.iloc[:,-1]\n",
    "    df_X = df.iloc[:,0:-1]\n",
    "    X_train,X_test,Y_train,Y_test=train_test_split(df_X,df_Y,test_size=0.5)\n",
    "    sample_len = len(X_train)\n",
    "    epochs = 1\n",
    "    #\"[1,2,3] 0.25 [1,2,1] 1\" input format\n",
    "    \"\"\"b=sys.argv[1]\n",
    "    c=b.split(\"[\")\n",
    "    d=c[1].split(\"]\")\n",
    "    e=d[0].split(\",\")\n",
    "    hidden=[]\n",
    "    e=['1','2','3']\n",
    "    for i in range (len(e)):\n",
    "        hidden.append(int(e[i]))\n",
    "\n",
    "    learningrate=float(sys.argv[2])\n",
    "    b1=sys.argv[3]\n",
    "    f=b1.split(\"[\")\n",
    "    g=f[1].split(\"]\")\n",
    "    h=g[0].split(\",\")\n",
    "    activation=[]\n",
    "    for j in range (len(h)):\n",
    "        activation.append(int(h[j]))\n",
    "    lossfunction=int(sys.argv[4])\n",
    "    print(hidden)\n",
    "    print(learningrate)\n",
    "    print(activation)\n",
    "    print(lossfunction)\n",
    "    \"\"\"\n",
    "    hiddenlayers=[2]\n",
    "    numoutputs=1\n",
    "    activationfunc = [1,1]\n",
    "    alpha = 0.25\n",
    "    lossfunc =1\n",
    "    step = 2 # Need to add this as arg as well and epochs\n",
    "    epochs = 1000\n",
    "    mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, step, epochs)\n",
    "    pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "    \n",
    "    epoch_test = []\n",
    "    for i in range(0, 10000, 5000): \n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, step, i)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        epoch_test.append((i,cost))\n",
    "    \n",
    "    steps_test = []\n",
    "    for i in range(1,len(X_train), 20): \n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, lossfunc, i, epochs)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        steps_test.append((i,cost))\n",
    "        \n",
    "    activation_static_test = []\n",
    "    for i in range(1,4):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,[i for x in range(len(hiddenlayers) + 1)], alpha, lossfunc, step, epochs)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        activation_static_test.append((i,cost))\n",
    "        alpha_test = []\n",
    "\n",
    "    for i in np.arange(0,1, 0.10):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , i, lossfunc, step, epochs)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        alpha_test.append((i,cost))\n",
    "    lossfunc_test = []\n",
    "    for i in range(1,3):\n",
    "        mlp = training(X_train, hiddenlayers, numoutputs,activationfunc , alpha, i, step, epochs)\n",
    "        pred_val,loss, cost = mlp.predict_values(X_test,Y_test)\n",
    "        lossfunc_test.append((i,cost))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Hidden layers/neurons, dynamic activation     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 4.2164,  9.4607, -4.9288, -5.2366]]),\n",
       " array([[0.97703005, 0.99997909]]),\n",
       " array([[0.84406847]])]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 4.2164  ,  9.4607  , -4.9288  , -5.2366  ],\n",
       "        [-0.94255 ,  0.039307, -0.24192 ,  0.31593 ]]),\n",
       " array([[0.97703005, 0.99997909],\n",
       "        [0.65878376, 0.61382469]]),\n",
       " array([[0.84406847],\n",
       "        [0.80997378]])]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>4.2164</td>\n",
       "      <td>9.4607</td>\n",
       "      <td>-4.9288</td>\n",
       "      <td>-5.2366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1       2       3\n",
       "327  4.2164  9.4607 -4.9288 -5.2366"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/smiroshnikova/Desktop/trial.csv',header=None)\n",
    "df_Y = df.iloc[:,-1]\n",
    "df_X = df.iloc[:,0:-1]\n",
    "sample_len = len(df)\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(df_X,df_Y,test_size=0.2)\n",
    "    #\"[1,2,3] 0.25 [1,2,1] 1\" input format\n",
    "# b=sys.argv[1]\n",
    "# c=b.split(\"[\")\n",
    "# d=c[1].split(\"]\")\n",
    "# e=d[0].split(\",\")\n",
    "# hidden=[]\n",
    "# e=['1','2','3']\n",
    "# for i in range (len(e)):\n",
    "#     hidden.append(int(e[i]))\n",
    "    \n",
    "# learningrate=float(sys.argv[2])\n",
    "# b1=sys.argv[3]\n",
    "# f=b1.split(\"[\")\n",
    "# g=f[1].split(\"]\")\n",
    "# h=g[0].split(\",\")\n",
    "# activation=[]\n",
    "# for j in range (len(h)):\n",
    "#     activation.append(int(h[j]))\n",
    "# lossfunction=int(sys.argv[4])\n",
    "# print(hidden)\n",
    "# print(learningrate)\n",
    "# print(activation)\n",
    "# print(lossfunction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "3  0.2  0.8\n",
       "2  0.1  0.9\n",
       "0  0.1  0.9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "1  0.1  0.9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
